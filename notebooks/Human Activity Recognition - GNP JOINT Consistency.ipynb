{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition - SSL JOINT Consistency\n",
    "\n",
    "Last Update : 24 July 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 8\n",
    "# Nota Bene : notebooks don't deallocate GPU memory\n",
    "IS_FORCE_CPU = False # can also be set in the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/master\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(600000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 600 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
       ".prompt display:none;}  </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autosave 600\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# CENTER PLOTS\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"\"\" <style> .output_png {display: table-cell; text-align: center; margin:auto; }\n",
    ".prompt display:none;}  </style>\"\"\"))\n",
    "\n",
    "import os\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"notebooks\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skssl.transformers.neuralproc.datasplit import CntxtTrgtGetter, GetRandomIndcs, get_all_indcs\n",
    "from utils.data.tsdata import get_timeseries_dataset, SparseMultiTimeSeriesDataset\n",
    "\n",
    "get_cntxt_trgt_test = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.1, max_n_indcs=0.5),\n",
    "                                     targets_getter=get_all_indcs,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt_feat = CntxtTrgtGetter(contexts_getter=get_all_indcs,\n",
    "                                     targets_getter=get_all_indcs,\n",
    "                                     is_add_cntxts_to_trgts=False)  # don't context points to tagrtes\n",
    "\n",
    "get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                 targets_getter=GetRandomIndcs(min_n_indcs=0.5, max_n_indcs=0.99),\n",
    "                                 is_add_cntxts_to_trgts=False)  # don't context points to tagrtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_both = get_timeseries_dataset(\"har\")(split=\"both\")\n",
    "\n",
    "def cntxt_trgt_collate(get_cntxt_trgt, is_repeat_batch=False):\n",
    "    def mycollate(batch):\n",
    "        min_length = min([v.size(0) for b in batch for k,v in b[0].items() if \"X\" in k])\n",
    "        # chose first min_legth of each (assumes that randomized)\n",
    "        \n",
    "        batch = [({k:v[:min_length, ...] for k,v in b[0].items()}, b[1]) for b in batch]        \n",
    "        collated = torch.utils.data.dataloader.default_collate(batch)\n",
    "        \n",
    "        X = collated[0][\"X\"]\n",
    "        y = collated[0][\"y\"]\n",
    "        \n",
    "        if is_repeat_batch:\n",
    "            \n",
    "            X = torch.cat([X,X], dim=0)\n",
    "            y = torch.cat([y,y], dim=0)\n",
    "            collated[1] = torch.cat([collated[1], collated[1]], dim=0) # targets\n",
    "        \n",
    "        collated[0][\"X\"], collated[0][\"y\"], collated[0][\"X_trgt\"], collated[0][\"y_trgt\"] = get_cntxt_trgt(X, y)\n",
    "        \n",
    "        return collated\n",
    "    return mycollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DIM = 1  # 1D spatial input (although actually 2 but the first is for sparse channels)\n",
    "Y_DIM = data_both.data.shape[-1] # multiple channels\n",
    "N_TARGETS = len(np.unique(data_both.targets))\n",
    "\n",
    "sampling_percentages = [0.05, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "label_percentages = [N_TARGETS, N_TARGETS*2, 0.01, 0.05, 0.1, 0.3, 0.5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from skssl.transformers import GlobalNeuralProcess, NeuralProcessLoss, AttentiveNeuralProcess\n",
    "from skssl.utils.helpers import rescale_range\n",
    "from skssl.predefined import UnetCNN, CNN, MLP, SparseSetConv, SetConv, MlpRBF, GaussianRBF, BatchSparseSetConv\n",
    "from skssl.transformers.neuralproc.datasplit import precomputed_cntxt_trgt_split\n",
    "from utils.helpers import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "models = {}\n",
    "\n",
    "unet = partial(UnetCNN,\n",
    "               Conv=torch.nn.Conv1d,\n",
    "               Pool=torch.nn.MaxPool1d,\n",
    "               upsample_mode=\"linear\",\n",
    "               n_layers=18,\n",
    "               is_double_conv=True,\n",
    "               is_depth_separable=True,\n",
    "               Normalization=torch.nn.BatchNorm1d,\n",
    "               is_chan_last=True,\n",
    "               bottleneck=None,\n",
    "               kernel_size=7,\n",
    "               max_nchannels=256,\n",
    "              is_force_same_bottleneck=True,\n",
    "               _is_summary=True,\n",
    "              )\n",
    "\n",
    "kwargs = dict(x_dim=X_DIM, \n",
    "              y_dim=Y_DIM,\n",
    "              min_std=5e-3,\n",
    "                n_tmp_queries=128,\n",
    "                r_dim=64,\n",
    "              keys_to_tmp_attn=partial(SetConv, RadialBasisFunc=GaussianRBF),\n",
    "              TmpSelfAttn=unet,\n",
    "              tmp_to_queries_attn=partial(SetConv, RadialBasisFunc=GaussianRBF),\n",
    "              is_skip_tmp=False,\n",
    "              is_use_x=False,\n",
    "              get_cntxt_trgt=precomputed_cntxt_trgt_split,\n",
    "              is_encode_xy=False,\n",
    "             Classifier=partial(MLP, input_size=256+Y_DIM*4, output_size=N_TARGETS, \n",
    "                                dropout=0.5, hidden_size=128, n_hidden_layers=3, is_res=True))\n",
    "\n",
    "models[\"ssl_classifier_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs)\n",
    "\n",
    "kwargs_bis = deepcopy(kwargs)\n",
    "kwargs_bis[\"Classifier\"] = None\n",
    "\n",
    "models[\"transformer_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssl_classifier_gnp_large_shared_bottleneck - N Param: 1078238\n",
      "transformer_gnp_large_shared_bottleneck - N Param: 1006936\n"
     ]
    }
   ],
   "source": [
    "from utils.helpers import count_parameters\n",
    "for k,v in models.items():\n",
    "    print(k, \"- N Param:\", count_parameters(v()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntbks_helpers import train_models_\n",
    "from skorch.dataset import CVSplit\n",
    "from utils.data.ssldata import get_train_dev_test_ssl\n",
    "\n",
    "N_EPOCHS = 100 \n",
    "BATCH_SIZE = 32\n",
    "IS_RETRAIN = True # if false load precomputed\n",
    "chckpnt_dirname=\"results/challenge/har/\"\n",
    "\n",
    "from skssl.utils.helpers import HyperparameterInterpolator\n",
    "\n",
    "n_steps_per_epoch = len(data_both)//BATCH_SIZE\n",
    "get_lambda_clf=HyperparameterInterpolator(1e-5, 10, N_EPOCHS*n_steps_per_epoch, \n",
    "                              start_step=n_steps_per_epoch*10, mode=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training 50%har_100%lab/ssl_classifier_gnp_large_shared_bottleneck ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  epoch    train_loss    valid_acc    valid_loss    cp      dur\n",
      "-------  ------------  -----------  ------------  ----  -------\n",
      "      1        \u001b[36m7.9444\u001b[0m       \u001b[32m0.6736\u001b[0m        \u001b[35m0.6922\u001b[0m     +  21.1911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      2        \u001b[36m4.1732\u001b[0m       \u001b[32m0.6858\u001b[0m        \u001b[35m0.6379\u001b[0m     +  21.1447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      3        \u001b[36m2.5341\u001b[0m       \u001b[32m0.7262\u001b[0m        \u001b[35m0.5816\u001b[0m     +  21.0974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      4        \u001b[36m2.0458\u001b[0m       0.6776        0.6560        21.3549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      5        \u001b[36m1.6721\u001b[0m       \u001b[32m0.7408\u001b[0m        0.6184        21.2895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      6        \u001b[36m1.3085\u001b[0m       0.6756        0.9155        20.7325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      7        \u001b[36m1.2530\u001b[0m       \u001b[32m0.7842\u001b[0m        0.6010        20.2866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      8        \u001b[36m0.8664\u001b[0m       0.7621        0.6917        21.3025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "      9        \u001b[36m0.6747\u001b[0m       0.7455        0.7645        21.3654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     10        \u001b[36m0.4369\u001b[0m       \u001b[32m0.8785\u001b[0m        \u001b[35m0.4268\u001b[0m     +  21.3988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     11        \u001b[36m0.3739\u001b[0m       0.8185        0.4724        21.5946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     12        \u001b[36m0.0097\u001b[0m       0.7472        0.6584        21.5670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     13       \u001b[36m-0.0173\u001b[0m       0.8653        \u001b[35m0.3569\u001b[0m     +  21.4773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     14       \u001b[36m-0.0842\u001b[0m       \u001b[32m0.8975\u001b[0m        \u001b[35m0.3147\u001b[0m     +  21.4850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     15       \u001b[36m-0.2122\u001b[0m       0.8860        \u001b[35m0.2988\u001b[0m     +  21.4199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     16       \u001b[36m-0.4549\u001b[0m       0.8188        0.4709        21.5196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     17       -0.3409       \u001b[32m0.9043\u001b[0m        \u001b[35m0.2766\u001b[0m     +  21.4568\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     18       \u001b[36m-0.8202\u001b[0m       \u001b[32m0.9128\u001b[0m        \u001b[35m0.2273\u001b[0m     +  21.4954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     19       \u001b[36m-1.1089\u001b[0m       0.8741        0.3188        21.4360\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     20       \u001b[36m-1.2847\u001b[0m       \u001b[32m0.9206\u001b[0m        \u001b[35m0.2208\u001b[0m     +  21.6017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     21       \u001b[36m-1.3895\u001b[0m       \u001b[32m0.9260\u001b[0m        \u001b[35m0.2002\u001b[0m     +  20.5840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     22       \u001b[36m-1.4829\u001b[0m       0.8914        0.3282        21.5256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     23       \u001b[36m-1.6173\u001b[0m       0.9203        0.2246        21.7182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     24       -1.6163       \u001b[32m0.9460\u001b[0m        \u001b[35m0.1531\u001b[0m     +  21.4484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     25       \u001b[36m-1.7375\u001b[0m       0.9406        0.1668        21.5160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     26       \u001b[36m-1.8691\u001b[0m       0.9345        0.1657        21.4642\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     27       \u001b[36m-2.0489\u001b[0m       0.9291        0.1668        21.5087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     28       \u001b[36m-2.0812\u001b[0m       0.9247        0.2318        21.4466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     29       \u001b[36m-2.1203\u001b[0m       0.9342        0.2202        21.5094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "     30       \u001b[36m-2.2372\u001b[0m       0.9433        0.1544        21.8175\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e901632d65ef46d3a3941254a81a70d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [0.5]:\n",
    "    for label_perc in [1]:\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                get_lambda_clf=lambda: get_lambda_clf(True),\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,t in data_trainers.items():\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [0.5]:\n",
    "    for label_perc in [1]:\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_no_entropy\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                is_entropies=False,\n",
    "                                                get_lambda_clf=lambda: get_lambda_clf(True),\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    k += \"_no_entropy\"\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [0.5]:\n",
    "    for label_perc in [1]:\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_no_consist\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                get_lambda_clf=lambda: get_lambda_clf(True),\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                is_consistency=False,\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    k += \"_no_consist\"\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without N Max Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [0.5]:\n",
    "    for label_perc in [1]:\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_no_cntxt_scaling\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                get_lambda_clf=lambda: get_lambda_clf(True),\n",
    "                                                n_max_elements=None,\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    k += \"_no_cntxt_scaling\"\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Lambda Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [0.5]:\n",
    "    for label_perc in [1]:\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_no_lambda_clf\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    k += \"_no_lambda_clf\"\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "Note that no get_lambda_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY INITALIZE TO BE ABLE TO LOAD\n",
    "models[\"ssl_classifier_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs)()\n",
    "\n",
    "kwargs_bis = deepcopy(kwargs)\n",
    "kwargs_bis[\"Classifier\"] = None\n",
    "\n",
    "models[\"transformer_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs_bis)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all transformers\n",
    "loaded_models = {}\n",
    "for sampling_perc in sampling_percentages:\n",
    "    for k, m in models.items():\n",
    "        if \"transformer\" not in k:\n",
    "            continue\n",
    "            \n",
    "        out = train_models_({\"{}%har\".format(int(sampling_perc*100)): \n",
    "                                            (None, None)}, \n",
    "                              {k :m },\n",
    "                               chckpnt_dirname=chckpnt_dirname,\n",
    "                               is_retrain=False)\n",
    "        \n",
    "        pretrained_model = out[list(out.keys())[0]].module_\n",
    "        model_dict = models[k.replace(\"transformer\", \"ssl_classifier\")].state_dict()\n",
    "        model_dict.update(pretrained_model.state_dict())\n",
    "        models[k.replace(\"transformer\", \"ssl_classifier\")].load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [0.5]:\n",
    "    for label_perc in [1]:\n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k+\"_finetune\" :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=IS_RETRAIN,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    k += \"_finetune\"\n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Sampling Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO BACK TO NO FINETUNING\n",
    "models[\"ssl_classifier_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs)\n",
    "\n",
    "kwargs_bis = deepcopy(kwargs)\n",
    "kwargs_bis[\"Classifier\"] = None\n",
    "\n",
    "models[\"transformer_gnp_large_shared_bottleneck\"] = partial(GlobalNeuralProcess, **kwargs_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in sampling_percentages:\n",
    "    for label_perc in [1]:\n",
    "        is_retrain = False if sampling_perc == 0.5 and label_perc == 1 else IS_RETRAIN # already computed before\n",
    "        \n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                get_lambda_clf=lambda: get_lambda_clf(True),\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=is_retrain,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Label Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Freezer, LRScheduler\n",
    "\n",
    "data_trainers = {}\n",
    "\n",
    "for sampling_perc in [0.5]:\n",
    "    for label_perc in label_percentages:\n",
    "        is_retrain = False if sampling_perc == 0.5 and label_perc == 1 else IS_RETRAIN # already computed before\n",
    "        \n",
    "        data_train, _, data_test = get_train_dev_test_ssl(\"har\", \n",
    "                                                          n_labels=label_perc, \n",
    "                                                          data_perc=sampling_perc, \n",
    "                                                          dev_size=0)\n",
    "        \n",
    "        # add test as unlabeled data\n",
    "        data_train.data = np.concatenate([data_train.data, data_test.data], axis=0)\n",
    "        data_train.targets = np.concatenate([data_train.targets, -1*np.ones_like(data_test.targets)], axis=0)\n",
    "        data_train.indcs = np.concatenate([data_train.indcs, data_test.indcs], axis=0)\n",
    "        \n",
    "        data_trainers.update(train_models_({\"{}%har_{}%lab\".format(int(sampling_perc*100), int(label_perc*100)): \n",
    "                                            (data_train, data_test)}, \n",
    "                              {k :m for k,m in models.items() if \"ssl_classifier\" in k}, \n",
    "                              criterion=partial(NeuralProcessLoss, \n",
    "                                                ssl_loss=\"both\",\n",
    "                                                distance=\"jsd\",\n",
    "                                                get_lambda_clf=lambda: get_lambda_clf(True),\n",
    "                                                n_max_elements=int(128*sampling_perc),\n",
    "                                                ),\n",
    "                                patience=15,\n",
    "                              chckpnt_dirname=chckpnt_dirname,\n",
    "                              max_epochs=N_EPOCHS,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              is_retrain=is_retrain,\n",
    "                              callbacks=[],\n",
    "                              #callbacks=[Freezer(lambda x: not x.startswith('classifier'))],\n",
    "                              iterator_train__collate_fn=cntxt_trgt_collate(get_cntxt_trgt = CntxtTrgtGetter(contexts_getter=GetRandomIndcs(min_n_indcs=0.01, max_n_indcs=0.5),\n",
    "                                                                                                     targets_getter=get_all_indcs,\n",
    "                                                                                                     is_add_cntxts_to_trgts=False), \n",
    "                                                                            is_repeat_batch=True),  \n",
    "                              iterator_valid__collate_fn=cntxt_trgt_collate(get_cntxt_trgt_feat),\n",
    "                                          ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if bad has to try freezing again and smaller params\n",
    "for k,t in data_trainers.items(): \n",
    "    for e, h in enumerate(t.history[::-1]):\n",
    "        if h[\"valid_loss_best\"]:\n",
    "            print(k, \"epoch:\", len(t.history)-e, \n",
    "                  \"val_loss:\", h[\"valid_loss\"], \n",
    "                  \"val_acc:\", h[\"valid_acc\"])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.9304 best without n max elements\n",
    "* 0.9277: jsd | no 0.1 scale | n_max_elements | 100 sampels | 0.05 entropies\n",
    "* 0.9857 : jsd | 0.1 scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze\n",
    "* 0.9623 : jsd | no scale | n_max_elements | 100 sampels | 0.01 entropies | no freeze | no pretrain | [0.01,0.5] | linear interpolator (1,5)\n",
    "\n",
    "\n",
    "* 0.9671 : jsd | no scale ? | n_max_elements | 100 sampels | 0.05 entropies | no freeze | no pretrain | [0.1,0.5]\n",
    "* 0.9365 : jsd | no scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze | no pretrain | [0.01,0.5] | linear interpolator\n",
    "* 0.9824 : jsd | no scale | n_max_elements | 100 sampels | 0.01 entropies | no freeze | no pretrain | [0.01,0.5] | linear interpolator\n",
    "\n",
    "\n",
    "* 0.9844 : jsd | no 0.1 scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze\n",
    "* 0.9817 : jsd | no 0.1 scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze | cntxt [0.01,0.5]\n",
    "\n",
    "\n",
    "* 0.9627 : jsd | no scale | n_max_elements | 100 sampels | 0.01 entropies | no freeze | no pretrain | [0.01,0.5]\n",
    "* 0.9572 : jsd | no scale | n_max_elements | 100 sampels | 0.01 entropies | no freeze | no pretrain | [0.01,0.9] | linear interpolator\n",
    "\n",
    "\n",
    "* 0.9321: jsd | no 0.1 scale | n_max_elements | 100 sampels\n",
    "* 0.9365: jsd | no 0.1 scale | n_max_elements | 100 sampels | 0.1 entropies\n",
    "\n",
    "\n",
    "* 0.9450 : jsd | 0.2 scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze | no pretrain | [0.01,0.5]\n",
    "* 0.9315 : jsd | no scale | n_max_elements | 100 sampels | 0.05 entropies | no freeze | no pretrain | [0.01,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading 100%har/transformer_gnp_large_shared_bottleneck ---\n",
      "\n",
      "100%har/transformer_gnp_large_shared_bottleneck best epoch: 86 val_loss: -8.16725208180622\n"
     ]
    }
   ],
   "source": [
    "# load all transformers\n",
    "loaded_models = {}\n",
    "for sampling_perc in sampling_percentages:\n",
    "    for k, m in models.items():\n",
    "        if \"transformer\" not in k:\n",
    "            continue\n",
    "            \n",
    "        out = train_models_({\"{}%har\".format(int(sampling_perc*100)): \n",
    "                                            (None, None)}, \n",
    "                              {k :m },\n",
    "                               chckpnt_dirname=chckpnt_dirname,\n",
    "                               is_retrain=False)\n",
    "        \n",
    "        pretrained_model = out[list(out.keys())[0]].module_\n",
    "        model_dict = models[k.replace(\"transformer\", \"ssl_classifier\")].state_dict()\n",
    "        model_dict.update(pretrained_model.state_dict())\n",
    "        models[k.replace(\"transformer\", \"ssl_classifier\")].load_state_dict(model_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal, Categorical, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([0.2, 0.8], requires_grad=True)\n",
    "t2 = torch.tensor([0.7, 0.3], requires_grad=True)\n",
    "#torch.softmax(t2, -1)\n",
    "#torch.softmax(t1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = (t1 + t2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon_div(p1, p2):\n",
    "    p_avg = (p1 + p2) / 2\n",
    "    mask = (p_avg != 0).float()\n",
    "    # set to 0 p when M is 0 (because mean can only be 0 is vectors weree, but\n",
    "    # this is not the case due to numerical issues)\n",
    "    M = Categorical(probs=p_avg)\n",
    "    return ((kl_divergence(Categorical(probs=p1 * mask), M) +\n",
    "             kl_divergence(Categorical(probs=p2 * mask), M)) / 2)\n",
    "\n",
    "def yann_div(t1, t2):\n",
    "    M = (t1 + t2) / 2\n",
    "    return torch.min(kl_divergence(Categorical(probs=t1), Categorical(M)) + \n",
    "               kl_divergence(Categorical(probs=t2), Categorical(M)))\n",
    "\n",
    "def csiszar_dist(t1, t2):\n",
    "    M = (t1 + t2) / 2\n",
    "    return ((kl_divergence(Categorical(M), Categorical(probs=t1)\n",
    "                ) + kl_divergence(Categorical(M), Categorical(probs=t2)))/2)#**0.5\n",
    "\n",
    "def total_var(t1, t2):\n",
    "    return (t1 - t2).abs().sum(-1) / 2\n",
    "\n",
    "def bhattacharyya_dist(t1, t2):\n",
    "    return -torch.log((t1 * t2).sqrt().sum(-1))\n",
    "\n",
    "def hellinger_dist(t1, t2):\n",
    "    return (t1.sqrt() - t2.sqrt()).pow(2).sum(-1).sqrt() / (2**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.0, 1] [1, 0.0]\n",
      "yd 1.3862941265106201\n",
      "cd inf\n",
      "tv 1.0\n",
      "jsd 0.6931470632553101\n",
      "bd inf\n",
      "hd 1.0\n",
      "\n",
      "[0.5, 0.5] [0.4, 0.6]\n",
      "yd 0.010118838399648666\n",
      "cd 0.005077211186289787\n",
      "tv 0.10000000894069672\n",
      "jsd 0.005059419199824333\n",
      "bd 0.005076696630567312\n",
      "hd 0.07116072624921799\n",
      "\n",
      "[0.5, 0.5] [0.5, 0.5]\n",
      "yd 0.0\n",
      "cd 0.0\n",
      "tv 0.0\n",
      "jsd 0.0\n",
      "bd -0.0\n",
      "hd 0.0\n",
      "\n",
      "[0.4, 0.6] [0.3, 0.7]\n",
      "yd 0.011017337441444397\n",
      "cd 0.005537144839763641\n",
      "tv 0.09999997913837433\n",
      "jsd 0.0055086687207221985\n",
      "bd 0.005531022325158119\n",
      "hd 0.07426819950342178\n",
      "\n",
      "[1.0, 1e-50] [1e-50, 1.0]\n",
      "yd 1.3862941265106201\n",
      "cd inf\n",
      "tv 1.0\n",
      "jsd 0.6931470632553101\n",
      "bd inf\n",
      "hd 1.0\n",
      "\n",
      "[0.1, 0.1, 0.8] [0.2, 0.2, 0.6]\n",
      "yd 0.048314452171325684\n",
      "cd 0.024884231388568878\n",
      "tv 0.19999998807907104\n",
      "jsd 0.024157226085662842\n",
      "bd 0.024637971073389053\n",
      "hd 0.15600308775901794\n",
      "\n",
      "[0.1, 0.1, 0.8] [0.6, 0.2, 0.2]\n",
      "yd 0.407856285572052\n",
      "cd 0.24531465768814087\n",
      "tv 0.6000000238418579\n",
      "jsd 0.203928142786026\n",
      "bd 0.2403273582458496\n",
      "hd 0.4622008800506592\n"
     ]
    }
   ],
   "source": [
    "for t1,t2 in [([0., 1], [1, 0.]), \n",
    "              ([0.5, 0.5], [0.4, 0.6]), \n",
    "              ([0.5, 0.5], [0.5, 0.5]), \n",
    "              ([0.4, 0.6], [0.3, 0.7]), \n",
    "              ([1-1e-50, 1e-50], [1e-50, 1-1e-50]), \n",
    "              ([0.1, 0.1, 0.8], [0.2, 0.2, 0.6]), \n",
    "              ([0.1, 0.1, 0.8], [0.6, 0.2, 0.2])]:\n",
    "    print()\n",
    "    print(t1, t2)\n",
    "    print(\"yd\", yann_div(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"cd\", csiszar_dist(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"tv\", total_var(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"jsd\", jensen_shannon_div(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"bd\", bhattacharyya_dist(torch.tensor(t1), torch.tensor(t2)).item())\n",
    "    print(\"hd\", hellinger_dist(torch.tensor(t1), torch.tensor(t2)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'pow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b983a55a3696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'pow'"
     ]
    }
   ],
   "source": [
    "t1.pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
