import math
import sys
import os
import logging

import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
from sklearn.gaussian_process import GaussianProcessRegressor
import h5py

from neuralproc.utils.helpers import rescale_range

__all__ = ["GPDataset"]

logging.basicConfig(level=logging.INFO)


class GPDataset(Dataset):
    """
    Dataset of functions generated by a gaussian process.

    Parameters
    ----------
    kernel : sklearn.gaussian_process.kernels or list
        The kernel specifying the covariance function of the GP. If None is
        passed, the kernel "1.0 * RBF(1.0)" is used as default.

    min_max : tuple of floats, optional
        Min and max point at which to evaluate the function (bounds).

    n_samples : int, optional
        Number of sampled functios contained in dataset.

    n_points : int, optional
        Number of points at which to evaluate f(x) for x in min_max.

    n_diff_kernel_hyp : int, optional
        How many randomly sampled kernel hyperparameters to use per epoch. If
        `n_diff_kernel_hyp = 1` all the samples will be sampled from a fixed kernel.
        If `n_diff_kernel_hyp = n_samples`, every sample will be generated from
        different kernel hyperparameters (will be slow). If not `1` the kernel
        hyperparameters will be uniformly sampled in the bounds `*_bounds` of the
        kernel (note that you should fix ALL hyperparameter bounds).

    save_file : string or tuple of strings, optional
        Where to save and load the dataset. If tuple `(file, group)`, save in
        the hdf5 under the given group. If `None` regenerate samples indefinitely.
        Note that if the saved dataset has been completely used,
        it will generate a new sub-dataset for every epoch and save it for future
        use.

    kwargs:
        Additional arguments to `GaussianProcessRegressor`.
    """

    def __init__(self,
                 kernel=(WhiteKernel(noise_level=.1, noise_level_bounds=(.1, .5)) +
                         RBF(length_scale=.4, length_scale_bounds=(.1, 1.))),
                 min_max=(-2, 2),
                 n_samples=1000,
                 n_points=100,
                 n_diff_kernel_hyp=1,
                 save_file=None,
                 logging_level=logging.INFO,
                 **kwargs):

        self.n_samples = n_samples
        self.n_points = n_points
        self.min_max = min_max
        self.n_diff_kernel_hyp = n_diff_kernel_hyp
        self._check_n_samples(n_samples)
        self.logger = logging.getLogger('GPDataset')
        self.logger.setLevel(logging_level)
        self.save_file = save_file

        self._idx_precompute = 0  # current index of precomputed data
        self._idx_chunk = 0  # current chunk (i.e. epoch)

        if n_diff_kernel_hyp == 1:
            # only fit hyperparam when predicting if using various hyperparam
            kwargs["optimizer"] = None
        self.generator = GaussianProcessRegressor(kernel=kernel,
                                                  alpha=0.005,  # numerical stability for preds
                                                  **kwargs)
        self.precompute_chunk_()

    def _check_n_samples(self, n_samples):
        if n_samples % self.n_diff_kernel_hyp != 0 and n_samples != 1:
            raise ValueError("n_samples={} has to be dividable by n_diff_kernel_hyp={} or 1.".format(n_samples, self.n_diff_kernel_hyp))

    def __len__(self):
        return self.n_samples

    def __getitem__(self, index):
        # doesn't use index because randomly gnerated in any case => sample
        # in order which enables to know when epoch is finished and regenerate
        # new functions
        self._idx_precompute += 1
        if self._idx_precompute == self.n_samples:
            self.precompute_chunk_()
        return self.data[self._idx_precompute], self.targets[self._idx_precompute]

    def get_samples(self, n_samples=None, test_min_max=None,
                    n_points=None, save_file=None, idx_chunk=None):
        """Return a batch of samples

        Parameters
        ----------
        n_samples : int, optional
            Number of sampled function (i.e. batch size). Has to be dividable
            by n_diff_kernel_hyp or 1. If `None` uses `self.n_samples`.

        test_min_max : float, optional
            Testing range. If `None` uses training one.

        n_points : int, optional
            Number of points at which to evaluate f(x) for x in min_max. If None
            uses `self.n_points`.

        save_file : string or tuple of strings, optional
            Where to save and load the dataset. If tuple `(file, group)`, save in
            the hdf5 under the given group. If `None` uses does not save.

        idx_chunk : int, optional
            Index of the current chunk. This is used when `save_file` is not None,
            and you want to save a single dataset through multiple calls to
            `get_samples`.
        """
        test_min_max = test_min_max if test_min_max is not None else self.min_max
        n_points = n_points if n_points is not None else self.n_points
        n_samples = n_samples if n_samples is not None else self.n_samples
        self._check_n_samples(n_samples)

        try:
            data, targets = _load_chunk(save_file, idx_chunk)
        except NotLoadedError:
            X = self._sample_features(test_min_max, n_points)
            targets = self._sample_targets(X, n_samples)
            data = self._postprocessing_features(X, n_samples)
            _save_chunk(data, targets, save_file, idx_chunk, logger=self.logger)

        return data, targets

    def precompute_chunk_(self):
        """Load or precompute and save a chunk (data for an epoch.)"""
        self._idx_precompute = 0
        self.data, self.targets = self.get_samples(save_file=self.save_file,
                                                   idx_chunk=self._idx_chunk)
        self._idx_chunk += 1

    def _sample_features(self, min_max, n_points):
        """Sample X with non uniform intervals, by sampling from and adding noise. """
        X = np.linspace(*min_max, n_points)
        # add noise (with standard deviation of "stepsize") to not be on a grid
        X += np.random.randn(*X.shape) * (min_max[1] - min_max[0]) / n_points
        # make sure that still in bound
        X = X.clip(min=min_max[0], max=min_max[1])
        # sort which is convenient for plotting
        X.sort()
        return X

    def _postprocessing_features(self, X, n_samples):
        """Convert the features to a tensor, rescale them to [-1,1] and expand."""
        n_points = len(X)
        X = torch.from_numpy(X)
        X = X.view(1, -1, 1).expand(n_samples, n_points, 1).float()
        X = rescale_range(X, self.min_max, (-1, 1))
        return X

    def _sample_targets(self, X, n_samples):
        n_points = len(X)
        if self.n_diff_kernel_hyp == 1:
            targets = self.generator.sample_y(X[:, np.newaxis], n_samples).transpose(1, 0)
        else:
            if n_samples == 1:
                self.sample_kernel_()
                targets = self.generator.sample_y(X[:, np.newaxis], n_samples
                                                  ).transpose(1, 0)
            else:
                targets = np.empty((n_samples, n_points))
                for i in range(self.n_diff_kernel_hyp):
                    self.sample_kernel_()
                    # interleaves all arrays (maybe better than concat and shuffle)
                    targets[i::self.n_diff_kernel_hyp, :
                            ] = self.generator.sample_y(X[:, np.newaxis],
                                                        n_samples // self.n_diff_kernel_hyp
                                                        ).transpose(1, 0)

        targets = torch.from_numpy(targets)
        targets = targets.view(n_samples, n_points, 1).float()
        return targets

    def sample_kernel_(self):
        """
        Modify inplace the kernel hyperparameters through uniform sampling in their
        respective bounds.
        """
        K = self.generator.kernel
        for hyperparam in K.hyperparameters:
            K.set_params(**{hyperparam.name: np.random.uniform(*hyperparam.bounds.squeeze())})


def _parse_save_file_chunk(save_file, idx_chunk):
    if save_file is None:
        save_file, save_group = None, None
    elif isinstance(save_file, tuple):
        save_file, save_group = save_file[0], save_file[1] + "/"
    elif isinstance(save_file, str):
        save_file, save_group = save_file, ""
    else:
        raise ValueError("Unsupported type of save_file={}.".format(save_file))

    if idx_chunk is not None:
        chunk_suffix = "_chunk_{}".format(idx_chunk)
    else:
        chunk_suffix = ""

    return save_file, save_group, chunk_suffix


class NotLoadedError(Exception):
    pass


def _load_chunk(save_file, idx_chunk):
    save_file, save_group, chunk_suffix = _parse_save_file_chunk(save_file, idx_chunk)

    if save_file is None or not os.path.exists(save_file):
        raise NotLoadedError()

    try:
        with h5py.File(save_file, 'r') as hf:
            data = torch.from_numpy(hf["{}data{}".format(save_group, chunk_suffix)][:])
            targets = torch.from_numpy(hf["{}targets{}".format(save_group, chunk_suffix)][:])
    except KeyError:
        raise NotLoadedError()

    return data, targets


def _save_chunk(data, targets, save_file, idx_chunk, logger=None):
    save_file, save_group, chunk_suffix = _parse_save_file_chunk(save_file, idx_chunk)

    if save_file is None:
        return  # don't save

    if logger is not None:
        logger.info("Saving group {} chunk {} for future use ...".format(save_group, idx_chunk))

    with h5py.File(save_file, 'a') as hf:
        hf.create_dataset("{}data{}".format(save_group, chunk_suffix), data=data.numpy())
        hf.create_dataset("{}targets{}".format(save_group, chunk_suffix), data=targets.numpy())
