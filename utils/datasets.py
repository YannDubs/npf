import math
import sys

import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process import GaussianProcessRegressor

from neuralproc.utils.helpers import rescale_range

__all__ = ["GPDataset"]


class GPDataset(Dataset):
    """
    Dataset of functions generated by a gaussian process.

    Parameters
    ----------
    kernel : sklearn.gaussian_process.kernels
        The kernel specifying the covariance function of the GP. If None is
        passed, the kernel "1.0 * RBF(1.0)" is used as default.

    min_max : tuple of floats, optional
        Min and max point at which to evaluate the function (bounds).

    n_samples : int, optional
        Number of sampled functios contained in dataset.

    n_points : int, optional
        Number of points at which to evaluate f(x) for x in min_max.

    kwargs:
        Additional arguments to `GaussianProcessRegressor`.
    """

    def __init__(self,
                 kernel=1. * RBF(length_scale=1.),
                 min_max=(-5, 5),
                 n_samples=1000,
                 n_points=100,
                 **kwargs):

        self.n_samples = n_samples
        self.n_points = n_points
        self.min_max = min_max
        self.generator = GaussianProcessRegressor(kernel=kernel,
                                                  alpha=0.001,  # make sure that can predict (numerical stability)
                                                  optimizer=None,  # don't fit kernel hyperparam
                                                  **kwargs)
        self.data, self.targets = self.precompute_data()

    def __len__(self):
        return self.n_samples

    def __getitem__(self, index):
        # doesn't use index because randomly gnerated in any case => sample
        # in order which enables to know when epoch is finished and regenerate
        # new functions
        self.counter += 1
        if self.counter == self.n_samples:
            self.data, self.targets = self.precompute_data()
        return self.data[self.counter], self.targets[self.counter]

    def precompute_data(self):
        self.counter = 0
        return self._precompute_helper(self.min_max, self.n_samples, self.n_points)

    def _precompute_helper(self, min_max, n_samples, n_points):
        # sample from a grid
        X = np.linspace(*min_max, n_points)
        # add noise (with standard deviation of "stepsize") to not be on a grid
        X += np.random.randn(*X.shape) * (min_max[1] - min_max[0]) / n_points
        # make sure that still in bound
        X = X.clip(min=min_max[0], max=min_max[1])
        # sort which is convenient for plotting
        X.sort()

        targets = self.generator.sample_y(X[:, np.newaxis], n_samples).transpose(1, 0)
        targets = torch.from_numpy(targets)
        targets = targets.view(n_samples, n_points, 1).float()

        X = torch.from_numpy(X)
        X = X.view(1, -1, 1).expand(n_samples, n_points, 1).float()
        # rescale features to [-1,1]
        # uses `self.min_max` like that possible to precompute extrapolation data
        X = rescale_range(X, self.min_max, (-1, 1))

        return X, targets

    def extrapolation_samples(self, n_samples=1, test_min_max=None, n_points=None):
        """Return a batch of extrapolation

        Parameters
        ----------
        n_samples : int, optional
            Number of sampled function (i.e. batch size).

        test_min_max : float, optional
            Testing range. If `None` uses training one.

        n_points : int, optional
            Number of points at which to evaluate f(x) for x in min_max. If None
            uses `self.n_points`.
        """
        if test_min_max is None:
            test_min_max = self.min_max
        n_points = n_points if n_points is not None else self.n_points
        return self._precompute_helper(test_min_max, n_samples, n_points)
